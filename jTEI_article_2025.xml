<?xml version="1.0" encoding="UTF-8"?>
<?xml-model href="http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_jtei.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?>
<?xml-model href="http://www.tei-c.org/release/xml/tei/custom/schema/relaxng/tei_jtei.rng" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" rend="jTEI">
  <teiHeader>
    <fileDesc>
      <titleStmt>
        <title type="main">Preserving LISTSERV Archive in TEI</title>
        <author xml:id="sb">
          <name>
            <forename>Syd</forename>
            <surname>Bauman</surname>
          </name>
          <affiliation>
            <roleName>Senior XML Programmer/Analyst</roleName>
            <affiliation>Northeastern University</affiliation>
          </affiliation>
          <email>s.bauman@northeastern.edu</email>
        </author>
        <author xml:id="ebb">
          <name>
            <forename>Elisa</forename>
            <surname>Beshero-Bondar</surname>
          </name>
          <affiliation>
            <roleName>Professor of Digital Humanities</roleName>
            <roleName>Director of the Digital Humanities Lab</roleName>
            <orgName>Penn State Erie</orgName>
          </affiliation>
          <email>eeb4@psu.edu</email>
        </author>
      </titleStmt>
      <publicationStmt>
        <publisher>TEI Consortium</publisher>
        <date>due 2025-03-03</date>
        <availability>
          <licence target="https://creativecommons.org/licenses/by/4.0/">
            <p>For this publication a Creative Commons Attribution 4.0 International
              license has been granted by the author(s) who retain full copyright.</p>
          </licence>
        </availability>
      </publicationStmt>
      <sourceDesc>
        <p>No source, born digital; based in large part on our <ref
        target="https://bit.ly/listserv2tei">presentation</ref> at the
        TEI Conference and Members’ Meeting 2024 in Buenos Aires.</p>
      </sourceDesc>
    </fileDesc>
    <encodingDesc>
      <projectDesc>
        <p>OpenEdition Journals -centre for open electronic publishing- is the platform for journals
          in the humanities and social sciences, open to quality periodicals looking to publish
          full-text articles online.</p>
      </projectDesc>
    </encodingDesc>
    <profileDesc>
      <langUsage>
        <language ident="en">en</language>
      </langUsage>
      <textClass>
        <keywords xml:lang="en">
          <term>TEI</term>
          <term>LISTSERV</term>
          <term>character</term>
          <term>email</term>
          <term>computer-mediated communication</term>
        </keywords>
      </textClass>
    </profileDesc>
    <revisionDesc>
      <change when="2025-02-08" who="#sb">Started jTEI article outline based on Buenos Aires talk</change>
    </revisionDesc>
  </teiHeader>
  <text>
    <front>
      <div type="abstract" xml:id="abstract">
        <p><q>Can the archives of an email list be stored in TEI?</q> This seems
        like a reasonable question, to which the answer, especially with the new
        <title level="a">Computer-mediated Communcation</title> chapter of the
        <title>TEI Guidelines</title>, should be a simple <q>yes</q>.</p>
        <p>In order to prove this, each co-author attempted to encode a subset
        of the archives of <ref target="https://lists.psu.edu/cgi-bin/wa?A0=TEI-L">TEI-L</ref>,
        the main email discussion list for TEI, in TEI. While our methods were wildly different,
        our conclusions are similar: the answer is <q>yes, but it is a <emph>lot</emph> harder
        than you might expect</q>.</p>
      </div>
    </front>
    <body>
      <div xml:id="introduction">
	<head>Introduction</head>
	<p>One of the goals of digital humanists, and one of the
	points of digitial humanities, is cultural memory. It is
	commonly a goal to preserve the results of humanistic
	endeavors in a manner that one hopes will be easy to access
	many years in the future, and allows for scholarly examination
	and inquiry. From the last quarter of the 20th century to the
	present, one venue for communication among humans has been
	email, and in particular the email discussion list. Thus it is
	to be expected that digital humanists will want to preserve
	email discussion lists in a manner that is likely to survive
	the ups and downs of the word processor market, and is
	ammenable to scholarly processing.</p>
	<p>What is not expected is that the Text Encoding Initiative
	would not provide a dedicated mechanism for encoding email,
	but it does not. TEI does provide mechanisms for encoding
	<emph>physical</emph> letters and chains of
	correspondence. There are quite a few similarities — like a
	letter, a piece of email is written by an author and sent to a
	recipient at a particular point in time. But physical letters
	do not have such a sharp demarcation between data &amp;
	metadata, rarely have multiple copies of the same content in
	different formats (typically HTML and plain text), and never
	suffer character encoding ambiguities. (Conversely, email does
	not have seals or stamps or any of the inconsistencies present
	in any set of manuscripts.)</p>
	<p>TEI also provides a (new) mechanism for encoding
	computer-mediated communication (CMC). It is intended,
	however, for systems like Twitter or Tik-Tok rather than for
	email. Thus it is not yet known how well its encoding system
	could be applied to email. Thus the impetus for this project —
	can TEI, with its new CMC chapter, be used to encode an email
	discussion list?</p>
	<p>We decided the TEI-L discussion list would make an
	excellent test case. It is a large resource that spans decades
	— the first post was on <date when="1990-01-08">Mon 08 Jan
	90<!-- while I was still living in Pasedena, 6 days after I
	got my MICP certification —sb --></date> — from mailing list
	software (LISTSERV) with which we are somewhat
	familiar. Furthermore, because we had recently moved the list
	from Brown University to Penn State University, getting access
	to the raw archives from its beginning to the day the list was
	moved would be trivial. But most of all, the list is
	(obviously) of significant interest to the TEI community. Its
	contents provide a treasure trove of insights into the early
	days of text encoding and digital humanities. (The list was
	started months before the first edition of P1, and years
	before the advent of XML.)</p>
	<!-- gold mine; heritage dig site; core sample; digital archaeology -->
        <p>Our initial thought was that converting the data to XML would be easy,
          and that most of our attention would be directed towards the more interesting
          problem of deciding exactly how to encode it in TEI. However, it turns
          out that both parts were problematic — it was very difficult to convert
          the data into XML, and in several cases there is no TEI encoding that is
          really satisfactory.</p>
        <p>Our two approaches were
          <list>
            <item>an automated conversion of mass data to a basic
            TEI representation for archiving and potential later
            enhancement</item>
            <item>a craft encoding of a short range of months or
            years worth of posts</item>
          </list>
        </p>
      </div>
      <div xml:id="data-retrieval">
        <head>Conversion from source to XML</head>
       <div xml:id="mass-conversion">
         <head>Mass conversion approach</head>
        <p>Our goal for the mass conversion approach was to
        programatically convert all of TEI-L from its inception up to
        2024-04-29 into some form of simple, basic TEI. That end date
        was chosen because it was the day the mailing list was moved
        from Brown University’s LISTSERV to Penn State University’s
        LISTSERV. This fact meant we could obtain the entire dataset
        directly from Brown listmaster without needing to worry about
        merging in data from PSU or about posts made just after we
        obtained the data. Peter DiCamillo, the Brown listmaster, sent
        us 412 separate files, one for each month, i.e.,
        <name>tei-l.log9001</name> through
        <name>tei-l.log2404</name>.</p>
        <p>Thus the first step was to rename the log files to use
        4-digit (as opposed to 2-digit) years so that they sorted into
        the correct order. This would allow them to be easily examined
        in a reasonable and, more importantly, would allow them to be
        combined into a single file in the correct order using one
        simple command: <code>cat tei-l.log* >
        TEI-L.txt</code>.<note>In fact, in a vain effort to avoid the
        character encoding problems described below, we actually used
        <code>iconv -f ISO-8859-1 -t UTF-8 -c tei-l.log* >
        TEI-L.txt</code>, but do not think it made any
        difference.</note></p>
	<div xml:id="post_delimiters">
	  <head>Delimters</head>
	  <p>Each log file provided by LISTSERV contains one or more
	  posts to the mailing list. For our dataset, the range was
	  1–420 posts per monthly log file, although in several cases
	  there is a month that has no postings, for which LISTSERV
	  did not generate a .log file at all.</p>
	  <p>Thus an early task was to figure out how posts are
	  separated in a log file. Upon looking at the dataset, it was
	  apparent in seconds that LISTSERV uses a string of 73 equal
	  signs (U+003D, <q>=</q>) in a row as a start-post
	  indicator. (That is, there is such a delimiter before the
	  first and every subsequent post in a file, but not after the
	  last.)</p>
	  <p>But, of course, there is nothing to stop a mailing list
	  user from including a string of 73 equal signs inside her
	  post. We first checked (out of curiosity) to see if any
	  strings of 73 equal signs were obviously <emph>not</emph>
	  post deliimters, because they did not occupy the entire
	  record. A search for the delimiter string (using just
	  <code>egrep -c</code>) revealed that it occurs 31,031 times
	  in the dataset; a search for the delimiter string anchored
	  to the begining and end of the line revealed that it occurs
	  alone on a line 30,998 times. So there are 33 cases of 73
	  equal signs in a row that do not occupy the entire record,
	  and thus are not start-of-post indicators. (It is likely
	  they exist as cases of posts being copied-and-pasted into
	  other posts.)</p>
	  <p>But are there any cases of 73 equal signs in a row that
	  do occupy the entire line, but are <emph>not</emph> intended
	  to be a post-begin delimiter? We know that every e-mail
	  message (and thus every post to an e-mail list) starts with
	  a sequence of metadata records. E-mail metadata records are
	  in a very specific format: the field name, which must be
	  composed of one or more of the printable 7-bit ASCII
	  characters except colon, followed by a colon, followed by a
	  field body. On a quick examination of the dataset, it
	  appears that the first field in every case is the
	  <code>Date:</code> field. So we searched for occurrences of
	  73 equal signs that start at the beginning of a line and are
	  followed immedieately by a newline followed by something
	  <emph>other</emph> than an uppercase <q>D</q>.<note>That is,
	  we searched for the regular expression
	  <code>^={73}\n[^D]</code>.</note> There are only three such
	  cases, so we just examined each of them.</p>
	  <p>The first is simply a case of cut-and-paste: on <date
	  when="1992-09-10">Thu 10 Sep 92</date> Wendy Plotkin, who
	  was at the time a PhD candidate in History at UIC and who
	  served as the TEI’s research assistant (1990–1999), posted
	  e-mail that contained a copy of e-mail from Professor Robert
	  Jones of the University of Illinois Urbana-Champaign to a
	  CETH mailing list, including a line of 73 equal signs which
	  was used to separate the part she wrote from the part
	  Professor Jones had sent.</p>
	  <p>The other two cases are both of two lines of 73 equal signs
	  in a row, with nothing between. Whether these indicate that the
	  preceding post ended with the same set of characters that is
	  used as the begin-post delimiter, or that there is a missing
	  post, we do not know.</p>
	  <p>So the result of this analysis was that a line of 73
	  equal signs in a row could be used reliable to indicate the
	  start of a new post, with one exception, which we simply
	  changed by hand to a line of 72 equal signs. (Which is
	  visually the same for a human reader, but distinctly
	  different to a computer looking for 73 of them.) We also had
	  to be prepared for the possibility of two such lines in a
	  row.</p>
	</div>
        <div xml:id="characters_illegal">
          <head>Illegal characters (for XML)</head>
          <p>Early e-mail systems used ASCII (7-bit) characters
          only. Thus there were 128 available code points (0–2⁷-1, or
          0–127). Of those 128 available code points, only 99 of them
          are legal XML characters. The remaining 29 characters (for
          which see <ptr type="crossref" target="#appendix01"/>) are
          all non-printing control characters, intended for
          controlling the behavior of a peripheral device such as a
          card punch or printer, or (in a few cases)
          whitespace.<note>It is quite reasonable, in our opinion, for
          an astute reader of the XML specification to think that
          production 14 implies that <emph>any</emph> character (other
          than <code>&lt;</code> and <code>&amp;</code>) is allowed as
          data content. But production 14 needs to be read within the
          limits already established by production 2 — which prohibits
          these control characters.</note> While neither of us is an
          expert on early e-mail systems, we suspect that the
          designers never intended these characters to be used in
          e-mail, but did not feel it necessary to do anything to
          prevent them from being used, as the average user of a
          computer had no way of typing them, and thus would find it
          very difficult to get them into an e-mail.<note>In fact,
          early definitions of e-mail messages assert they are just a
          sequence of ASCII characters in the range of 1–127; see <ref
	  type="bibl" target="#RFC2822"/> Later
          RFCs updated this to say <q><!-- uote source="#RFC5822"-->the use of US-ASCII control
          characters (values 1 through 8, 11, 12, and 14 through 31)
          is discouraged since their interpretation by receivers for
          display is not guaranteed.</q></note>
	</p>
          <p>Nonetheless, of the 29 characters that are not legal in
          XML, 17 of them occur in the TEI-L archives:
          <list>
            <item>64 occurrences in 34 posts before 2000 (which, on reflection, was not too surprising)</item>
            <item>38 occurrences in 11 posts after 2008 (which we did not expect at all)</item>
          </list>
          These characters were generally used not to represent the
          control characters to which they are mapped in the ASCII
          character set, but rather to represent various accented
          characters that were not part of the original (7-bit) ASCII
          set. Often these characters occurred within text that was
          likely copied-and-pasted from another source (e.g., from a
          conference announcement). The last use of a character that
          is illegal in XML was on 2016-03-13.</p>
	  <p>An input document that contains illegal characters
	  <emph>cannot</emph> be processed by XSLT, whether it is in
	  an XML file or a plain text file. And, of course, such
	  characters cannot occur in an XML file whether that file is
	  generated by XSLT, XQuery, Python, C, or by hand in a text
	  editor. In some of those languages, or using a text editor,
	  you could <emph>put</emph> them into a file that has a
	  <q>.xml</q> extension, but the file would not meet the
	  definition of an XML file, and thus could not be processed
	  by conforming XML software (other than to emit an error
	  message).</p>
	  <p>These control characters are being used incorrectly, and
	  are not allowed in XML, but they are not mal-formed or
	  illegal Unicode characters — they are perfectly legitimate
	  Unicode characters that are not allowed in XML. Thus
	  oXygen’s otherwise useful <name>Encoding errors
	  handling</name> feature is not helpful here, as it relates
	  to oXygen’s processing of mal-formed Unicode
	  characters. Likewise a utility like <name>iconv</name>,
	  which converts text from one character encoding to another,
	  is of little to no use — these control characters are
	  problematic no matter what character encoding is used to
	  represent them.</p>
	  <p>We immediately envisioned several possible solutions,
	  each of which would have to be carried out on the plain text
	  input files prior to processing into XML.
	  <list rend="ordered">
	    <item>Delete the problematic characters, leaving users to
	    figure out what was missing and where. This lossy
	    methodology seemed like a bad idea.</item>
	    <item>Replace each of the 102 occurrences of the 17
	    problematic characters with a single
	    <soCalled>warning</soCalled> character. This would allow
	    users to immediately know where there was a problem, but
	    would leave solving it (i.e., figuring out which character
	    belonged there) to the user. It requires, of course, that
	    we use a character that does not occur in the data. (One
	    obvious choice, for example, would be <q>⚠</q> (U+26A0),
	    but it already occurs three times in the input dataset.)
	    This is obviously better than just deleting the
	    characters, but still puts effort on the reader.</item>
	    <item>Replace the problematic characters with 17 different
	    <soCalled>warning</soCalled> characters. We believe some
	    users would find decoding this a lot easier, but others
	    would find it more difficult (because it did not seem to
	    be the case that there was always a 1:1 relationship
	    between the control character used and the desired
	    character). This also would mean that we would have to
	    find 17 characters that would be clear to the user, but do
	    not occur in the input dataset.</item>
	    <item>Replace the problematic characters with 17 different
	    <soCalled>warning</soCalled> <emph>strings</emph>. After
	    all, XML does have a replacement mechanism. So if we were
	    to replace each of the six occurences of, say, U+001A with
	    <q>⛑SUB⛑</q> in the input document, we could either leave
	    them that way for the user, or convert them to
	    <q>&amp;SUB;</q> and provide an appropriate declaration,
	    for example to a PUA character: <code>&lt;!ENTITY SUB
	    "&#xE503;"></code>. This would likely be easier for
	    readers than using a single character for each of the 17
	    problematic characters, but as with that solution readers
	    would be confused when there was not a 1:1 mapping from
	    the strings we provide and the likely intended
	    characters.</item>
	    <item>For each occurence of a problematic character, try
	    to figure out what character the author intended, and
	    replace it with that (proper UTF-8) character. While this
	    involves far more work than any of the above, we feel this
	    option results in documents that are easier for users to
	    process and read.  We considered four variations on this
	    theme, based on two binary features:
	    <list rend="bulleted">
	      <item>in each case, try contacting the author to ask
	      what was the intended character, or not;</item>
	      <item>in each case, just replace the problematic
	      character with our guess at the correct desired
	      character, or replace it with a <gi>choice</gi> encoding
	      which records both the original character (e.g., as
	      <code>&lt;orig>&lt;g type="CONTROL"
	      subtype="FS"/>&lt;/orig></code>) and our regularization
	      thereof (e.g., as
	      <code>&lt;reg>&#x00A3;&lt;/reg></code>).</item>
	    </list>
	    While we think that contacting the original authors is, in
	    some cases, beneficial, and believe that the
	    <gi>choice</gi> encoding is by far the best way of
	    representing the problematic characters, for our small
	    experiment we chose the easier route for each of these
	    feautres.<note>As it is, finding the problematic
	    characters, figuring out what each was likely supposed to
	    be (often by multiple visits to the <ref
	    target="https://webcf.waybackmachine.org/">Internet
	    Archive’s Wayback Machine</ref>), and replacing them took
	    multiple hours.</note></item>
	  </list>
	  </p>
	  <p>After replacement of those characters that are illegal in
	  XML by those characters we thought they should be, the input
	  dataset was easily read by XSLT. Converting that to useful,
	  readable XML presented some challenges, though.</p>
        </div>
	<div xml:id="conversion_complexities">
	  <head>Conversion complexities</head>
	  <div xml:id="Encoded-word_syntax">
	    <head>ASCII work-around: Encoded-word syntax</head>
	    <p>As mentioned above, early e-mail used only 7-bit ASCII
	    characters for the entire message. Later updates to the
	    specifications allowed for the body of the e-mail message
	    to use a different character set, but the e-mail headers
	    (that is, the lines of metadata at the top) are still to
	    this day limited to 7-bit ASCII. Most of the world’s
	    printable characters, of course, are not available in that
	    set of 96 printable characters. To allow for characters
	    outside the 7-bit ASCII set to be used in e-mail headers
	    (e.g., in the name of the sender or recipient of an
	    e-mail), an escape system called <term>encoded-word
	    syntax</term> may be employed.</p>
	    <p>Using this system, a word that contains a character
	    outside of 7-bit ASCII is encoded as
	    <code>=?charset?encoding?encoded text?=</code>, where
	    <term>charset</term> is the character encoding in which
	    the encoded text should be looked up,
	    <term>encoding</term> is either <q>Q</q> (for a
	    quoted-printable sytle escape mechanism) or <q>B</q> (for
	    a base64 style escape mechanism), and <term>encoded
	    text</term> is the string encoded using the escape
	    mechanism indicated. The character sets employed in this
	    dataset were as follows, using mixed case. (For example,
	    UTF-8 was often spelled <q>utf-8</q>.)
	    <table>
	      <row><cell>2,300</cell><cell>UTF-8</cell></row>
	      <row><cell>1,043</cell><cell>ISO-8859-1</cell></row>
	      <row><cell>383</cell><cell>ISO-8859-2</cell></row>
	      <row><cell>236</cell><cell>Windows-1252</cell></row>
	      <row><cell>132</cell><cell>ISO-8859-15</cell></row>
	      <row><cell>16</cell><cell>ISO-8859-5</cell></row>
	      <row><cell>5</cell><cell>KOI8-R</cell></row>
	      <row><cell>4</cell><cell>big5</cell></row>
	      <row><cell>3</cell><cell>ISO-2022-jp</cell></row>
	      <row><cell>3</cell><cell>Windows-1250</cell></row>
	      <row><cell>2</cell><cell>ISO-8859-4</cell></row>
	      <row><cell>2</cell><cell>Windows-1256</cell></row>
	      <row><cell>2</cell><cell>Windows-1251</cell></row>
	      <row><cell>1</cell><cell>Windows-1257</cell></row>
	      <row><cell>1</cell><cell>X-UNKNOWN</cell></row>
	      <row><cell>1</cell><cell>gb2312</cell></row>
	    </table>
	    Dramatically more word-encoded passages in this dataset
	    employ <q>Q</q> encoding over the <q>B</q> method: 3,397
	    (82.2%) and 737 (17.8%), respectively.</p>
	    <p>We recognized and considered the problem of passages
	    represented with word-encoded syntax. Our instinct is that
	    most digital humansists with an interest in this data
	    would be better served by the results of our conversion
	    process if we decoded these strings (for example,
	    converted <q><?syd put Piotr Q-word-endcoded here?></q> to
	    <q><?syd put Piotr’s name here?></q>), and that those
	    performing digital archaeology about how various
	    characters were represented by various historical email
	    systems would just as soon use the source data rather than
	    the TEI-encoded output of our process. We are not
	    convinced of this position, though, and did not have
	    anywhere near enough time in this pilot project to attempt
	    to write a word-encoded syntax decoder in XSLT.</p>
	  </div>
	  <div xml:id="memory">
	    <head>Java, and memory, and streams, oh my!</head>
	    <p>We quickly discovered that processing the entire ~151 MiB
	    of data at once requires a lot of RAM, more than Saxon gets
	    from the Java virtual machine by default. There are three
	    obvious solutions to this problem.
	    <list rend="numbered">
	      <item>Chop up the data: Rather than reading in a single
	      file of ~151 MiB of data, use perhaps 10 files of ~15
	      MiB each (or vice-versa); or even use the original 412
	      files (the largest of which is <?syd?> MiB) generated by
	      LISTSERV.</item>
	      <item>Use streaming XSLT: The whole point of the
	      <soCalled>streamability</soCalled> feature of XSLT 3.0
	      is to be able to process large, perhaps infinitely
	      large, datasets in memory.</item>
	      <item>Give the Java virtual machine more heap space.</item>
	    </list>
	    </p>
	    <p>Using smaller chunks of input seems like a very good
	    approach in the general case. We did not follow that route
	    simply for the sake of expediency: we had already put in
	    hours of effort on the combined ~151 MiB file of data
	    (searching for and changing characters illegal for XML),
	    and simply did not want to take the time to artificially
	    divide the data into multiple files and then change our
	    XSLT program so it would iterate over them. That said,
	    this would be a reasonable approach were it not for the
	    time constraints on our (unfunded) pilot project.</p>
	    <p>We <emph>think</emph>, but are not at all convinced,
	    that use of streaming XSLT would solve this problem.
	    Although the data we are reading in is not XML, it is
	    internally converted to XML and then processed in several
	    stages. It is likely that if these stages were streamable
	    the memory use would be dramatically decreased. However,
	    neither of us is particularly well versed in writing
	    steamable XSLT, and, perhaps more importantly, do not
	    (yet?) consider it a viable solution for digital
	    humanists, as the only XSLT processors in the world that
	    have this feature are payware.</p>
	    <p>So while we admit it may be rude to require a user of
	    our program to do so, we simply increased Java heap space
	    when running this transofrm (to 2 GiB).</p>
	</div>
       </div>
        <div xml:id="scraping">
          <head>Attempting to scrape from the web</head>
        </div> 
      </div>
      <div xml:id="mapping-to-TEI">
        <head>Mapping to TEI</head>
        <!--ebb: To start here. -->
        
      </div>
    </body>
    <back>
      <div type="bibliography">
        <!-- the bibliography for the article, organized as a series of <bibl> elements inside <listBibl> -->
        <listBibl>
          <bibl xml:id="RFC2822">
            Hey, Elisa — how do we cite an RFC?
            <idno type="DOI">10.17487/RFC2822</idno>.
          </bibl>
          <bibl xml:id="RFC5822">
            Hey, Elisa — how do we cite an RFC?
            <idno type="DOI">10.17487/RFC5822</idno>.
          </bibl>
        </listBibl>
      </div>
      <div type="appendix" xml:id="appendix01">
        <!-- put copy of https://tei-cmc-experiment.github.io/tei-cmc-experiment/ASCII_not_in_XML.html,
             converted to TEI, here. But note that some of those regexps look like they need fixing. -->
      </div>
    </back>
  </text>
</TEI>
